{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact Center Insights using Generative AI\n",
    "\n",
    "Most financial institutions have contact centers where customers can call in and ask for help. When a contact center agent connects with a customer they are asked to follow a set of guidelines and templates they need to follow. For financial services in certain countries there are regulations that agents need to follow.\n",
    "\n",
    "After each call agents need to perform certain actions: \n",
    "\n",
    "* produce a summary of the call and enter it into an internal CRM such as Salesforce or a case management or ticketing system\n",
    "* write an email correspondence with a summary of the call and list out the actions discussed\n",
    "\n",
    "\n",
    "Additionally, the call needs to reviewed by team leads and auditors to ensure that company standards and regulations are met. These actions include:\n",
    "* perform QA check\n",
    "* capture customer sentiment after the call\n",
    "\n",
    "\n",
    "## Solution Architecture\n",
    "\n",
    "![Workflow for analyzing post-call recordings](./9.1.jpg)\n",
    "\n",
    "Figure 9.1: Workflow showing steps involved in analyzing post-call recordings\n",
    "\n",
    "In this example, we will start with an example call transcript. For simplicity, we will skip the call transcription step assuming it has already happened. We will focus on how to use GenAI to perform various post-call analysis steps we discussed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the sample call transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required libraries\n",
    "!pip3 install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello, this is Lisa from National Bank. How are you today? \n",
      "Customer: I'm good. Thank you. I'm calling in relation to a problem I'm having with my account.\n",
      "Agent: I'm sorry to hear that. Before we begin, let me inform you that this call will be recorded for quality training and regulatory purposes. Is that ok? \n",
      "Customer: Yeah, that's ok. \n",
      "Agent: Great. Thank you. What's the problem you are facing?\n",
      "Customer: I am not able to login to my account and also not able to use my debit card. I have tried resetting my password but the issue persists.\n",
      "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to the website and try resetting the password from there and see if you get password reset email and try following the steps from there?\n",
      "Customer: Ok, i tried it and received the password reset mail. I have reset my password but the login still fails.\n",
      "Agent: Okay, from your login history i can see you tried few times last week and your account got locked out. You also tried few incorrect pin attempts on your debit card and that is how your debit card also got blocked. Can you confirm if you performed these actions?\n",
      "Customer: I did try to login multiple times and failed. but i did not use my debit card at all last week.\n",
      "Agent: Alright, can you confirm the last debit card transaction you did with the card?\n",
      "Customer: Okay, I used the card for buying groceries last week and did a purchase of around USD 50.\n",
      "Agent: Okay, i can see that in my system as well but then i see 4 failed debit card usage attempts somewhere in eastern europe over the last 4 days. Was that you?\n",
      "Customer: No that was not me.\n",
      "Agent: I see. It looks like you had an unauthorized usage on your card. I am going to permanently block your card and issue you a new one.\n",
      "Customer: Do I need to come to bank branch to get the new card?\n",
      "Agent: No, You will receive it by mail in next few days. Your debit card pin will be mailed separately to you.\n",
      "Customer: Okay, and what about the login issue?\n",
      "Agent: Let me reset your credentials from the backend. since your debit card was compromised, your account might also be at risk. So i am going to reset your account credentials. from the backend.\n",
      "Customer: How long will it take to get my new debit card and login credentials?\n",
      "Agent: For the debit card, It depends on the speed of mail delivery. but it usually takes up to 5 business days for both debit card and pin to arrive at your address.\n",
      "Customer: And what about new login credentials?\n",
      "Agent: Within next hour, you will receive an email on your registered email id with new login credentials. It will have your temporary password and link to login. Upon clicking the link, you will be asked to put in your temporary password. Then the page will force you to create a new password. As soon you enter a new password, your new credentials will active immediately for use.\n",
      "Customer: Alright, thanks for your help.\n",
      "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
      "Customer: No, that's all for now.\n",
      "Agent: Alright, have a great day and thanks for banking with us!\n"
     ]
    }
   ],
   "source": [
    "with open(\"./bank-call-centre-transcript.txt\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to call Llama 3 model \n",
    "import boto3\n",
    "\n",
    "\n",
    "def generate_conversation(bedrock_client,\n",
    "                     model_id,\n",
    "                     system_text,\n",
    "                     input_text):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_text (JSON) : The system prompt.\n",
    "        input text : The input message.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Generating message with model\", model_id)\n",
    "\n",
    "    # Message to send.\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    system_prompts = [{\"text\" : system_text}]\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.1\n",
    "\n",
    "    #Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def ask_llm(system_text, input_text):\n",
    "    \"\"\"\n",
    "    Generates text using a large language model (LLM).\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to generate text from.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    model_id = \"meta.llama3-70b-instruct-v1:0\"\n",
    "    aws_region = \"us-east-1\"\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime', region_name=aws_region)\n",
    "\n",
    "    response = generate_conversation(bedrock, model_id, system_text, input_text)\n",
    "    answers = response['output']['message']['content']\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 1: Call Summarization\n",
    "One common requirement for contact center agents is to create a detailed summarize the customer call which is recorded within various internal systems. We can use LLMs to summarize the call transcript using prompt engineering techniques. We have an example prompt below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "Transcript Summary:\n",
      " - I answered the call and introduced myself as Lisa from National Bank, ensuring the customer was aware that the call would be recorded for quality training and regulatory purposes.\n",
      "\n",
      " - The customer explained they were experiencing issues with their account, specifically being unable to log in and use their debit card, despite attempting to reset their password.\n",
      "\n",
      " - I guided the customer through troubleshooting steps, asking them to reset their password from the website and follow the password reset email instructions.\n",
      "\n",
      " - The customer reported that they had received the password reset email but were still unable to log in.\n",
      "\n",
      " - I reviewed the customer's login history and found that their account had been locked out due to multiple failed login attempts, and their debit card had been blocked due to incorrect PIN attempts.\n",
      "\n",
      " - The customer confirmed the failed login attempts but denied using their debit card for the incorrect PIN attempts.\n",
      "\n",
      " - I asked the customer to confirm their last debit card transaction, which they reported was a legitimate purchase of around USD 50 for groceries.\n",
      "\n",
      " - I informed the customer that I had found four failed debit card usage attempts in Eastern Europe, which they denied making.\n",
      "\n",
      " - I concluded that the customer's debit card had been compromised and decided to permanently block the card and issue a new one.\n",
      "\n",
      " - I assured the customer that they would receive the new debit card and PIN by mail within the next few days.\n",
      "\n",
      " - I also reset the customer's account credentials from the backend due to the potential risk of compromise.\n",
      "\n",
      " - The customer inquired about the timeframe for receiving the new debit card and login credentials, and I explained that the debit card and PIN would arrive within 5 business days, and the new login credentials would be emailed within the next hour.\n",
      "\n",
      " - I offered to assist the customer with any further issues, but they reported that their concerns had been addressed.\n",
      "\n",
      " - I ended the call by thanking the customer for banking with us and wishing them a great day.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_template = \"\"\"\n",
    "  Inside the <transcript></transcript> XML tags is a transcript from a phone call between a Banking Customer Advisor (also known as the Agent) and a Customer (also know as the Customer) regarding an online website related issue.\n",
    "\n",
    "  <transcript>$transcript</transcript>\n",
    "\n",
    "  Use only this transcript for context.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\"\n",
    "  Your task is to write a detailed summary of the call in dot point form, from the perspective of the Agent. Include a line break between each dot point.\n",
    "\n",
    "  Respond immediately without preamble, and output the response in the following example format:\n",
    "  \n",
    "  Transcript Summary:\n",
    "  - (summary dot points)\n",
    "\n",
    "  Assistant: OK, here is the summary:\n",
    "\"\"\"\n",
    "\n",
    "from string import Template\n",
    "system_prompt = Template(system_prompt_template).safe_substitute(transcript=transcript)\n",
    "\n",
    "output_lines = ask_llm(system_prompt, summary_prompt)\n",
    "for line in output_lines:\n",
    "    print(line['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 2: Creating case notes\n",
    "After a customer call, contact center agents often need to record the key points and notes in internal systems such as CRMs. Let's see how we can use prompt engineering to generate case notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "Case: Customer is experiencing issues with logging into their online account and using their debit card.\n",
      "\n",
      "Notes:\n",
      "  - Customer is unable to login to their online account and use their debit card despite resetting their password.\n",
      "\n",
      "  - Customer's account was locked out due to multiple incorrect login attempts and debit card was blocked due to incorrect PIN attempts.\n",
      "\n",
      "  - Unauthorized debit card transactions were detected in Eastern Europe.\n",
      "\n",
      "Actions:\n",
      "  - Permanently blocking the customer's debit card and issuing a new one to be mailed to the customer.\n",
      "\n",
      "  - Resetting the customer's account credentials from the backend due to potential account compromise.\n",
      "\n",
      "  - Customer will receive an email with temporary login credentials within the next hour, which will prompt them to create a new password.\n",
      "\n",
      "  - Customer will receive the new debit card and PIN by mail within 5 business days.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "case_note_prompt = \"\"\"\n",
    "  Write a concise summary of the call in dot point form. Be sure to:\n",
    "  1. Summarise the nature of the injury.\n",
    "  2. Any administrative steps being undertaken by the Agent.\n",
    "  3. Any follow up actions or next steps that were mentioned.\n",
    "  4. Write in 1st person from the perspective of the Agent.\n",
    "  5. Include a line break between each dot point.\n",
    "  6. Do not include preamble in your response. \n",
    "\n",
    "  Respond immediately without preamble, and output the response in the following format:\n",
    "\n",
    "  Case: (one sentence summary of the nature of the injury)\n",
    "\n",
    "  Notes:\n",
    "  - (include dot point summary of the nature of the injury, treatment, and current work circumstances)\n",
    "\n",
    "  Actions:\n",
    "  - (include dot point summary of any administrative steps, follow up actions, or next steps being undertaken by the Agent)\n",
    "\"\"\"\n",
    "\n",
    "output_lines = ask_llm(system_prompt, case_note_prompt)\n",
    "for line in output_lines:\n",
    "    print(line['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 3: Email correspondence\n",
    "After the call is finished, agents typically are required to summarize the discussion and confirm the follow-up actions in an email back to the customer. Here we see how we could generate an email correspondence using prompt engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "Subject: Summary of Our Conversation Regarding Your Account Issue\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "I wanted to take a moment to thank you for reaching out to us about the issues you've been experiencing with your online account and debit card. I apologize again for the inconvenience this has caused and appreciate the time you took to speak with me today.\n",
      "\n",
      "As we discussed, it appears that your account was locked out due to multiple failed login attempts, and your debit card was blocked after several incorrect PIN entries. Additionally, our system detected unauthorized usage on your debit card in Eastern Europe, which we take very seriously.\n",
      "\n",
      "To resolve these issues, I have taken steps to permanently block your current debit card and will be issuing a new one, which you should receive by mail within the next 5 business days. Your new debit card PIN will be mailed separately. I have also reset your account credentials from the backend, and you will receive an email with temporary login credentials within the next hour. Please follow the instructions in the email to create a new password, which will be active immediately.\n",
      "\n",
      "Please be assured that we are taking all necessary measures to ensure the security of your account. If you have any further questions or concerns, please don't hesitate to reach out to us.\n",
      "\n",
      "Once again, thank you for your patience and cooperation. We appreciate your business and look forward to continuing to serve you.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Lisa\n",
      "National Bank\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correspondence_prompt = \"\"\"\n",
    "  Draft a email from the Agent to the Customer summarising the conversation. Be sure to:\n",
    "  1. Thank the customer for their time\n",
    "  2. Highlight the issue discussed and recommended actions \n",
    "  3. Any administrative steps being undertaken by the Agent\n",
    "  4. Any follow up actions that were agreed upon\n",
    "  5. Use sentences not dot points.\n",
    "  6. Sign the letter off from National Bank\n",
    "\n",
    "  The tone of the letter should be personable, but professional. \n",
    "\n",
    "  Respond immediately without preamble.\n",
    "\n",
    "  Assistant: OK, here is the email:\n",
    "\"\"\"\n",
    "\n",
    "output_lines = ask_llm(system_prompt, correspondence_prompt)\n",
    "for line in output_lines:\n",
    "    print(line['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 4: Post-call Quality Checks\n",
    "Contact center agents have to follow various regulatory requirements and enterprise quality standards when engaging with customers. These requirements need to be validate post-call to ensure that standards are upheld. This is a highly manual task and because of the high volume in most contact centers this can't be performed on all calls, and only a sample of the calls are analysed. \n",
    "\n",
    "With the new GenAI techniques, we can automatically validate all calls based on our requirements and standards. Let's see how we can achieve this using prompt engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "Here are the answers to the questions:\n",
      "\n",
      "1. Did the Agent identify themself? - Y\n",
      "2. Did the Agent explicitly state they were conducting a 'privacy check'? - N\n",
      "3. Did the Agent mention that the call is being recorded? - Y\n",
      "4. Confirmed the nature of the customer problem? - Y\n",
      "5. Confirmed the customer preferred method of contact? - N\n",
      "6. Communicated follow-up actions? - Y\n",
      "\n",
      "Score percentage: (4/6) * 100% = 66.67%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_prompt = \"\"\"\n",
    "  Your task is to analyse the transcript and answer each of questions inside the <qa></qa> XML tags.  \n",
    "\n",
    "  <qa>\n",
    "  Did the Agent identify themself?\n",
    "  Did the Agent explicitly state they were conducting a 'privacy check'?\n",
    "  Did the Agent mention that the call is being recorded?\n",
    "  Confirmed the nature of the customer problem?\n",
    "  Confirmed the customer preferred method of contact?\n",
    "  Communicated follow-up actions?\n",
    "  </qa>\n",
    "\n",
    "  Answer with [Y] if the question can be answered (yes), or [N] if it cannot (no). \n",
    "\n",
    "  Based on your answer, calculate a score percentage. The formula for calculating the score percentage is:\n",
    "  <formula>\n",
    "  (number of questions answered with [Y]) / (total number of questions)\n",
    "  </formula>\n",
    "\n",
    "  Do not output in XML.\n",
    "\"\"\"\n",
    "\n",
    "output_lines = ask_llm(system_prompt, qa_prompt)\n",
    "for line in output_lines:\n",
    "    print(line['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 5: Question answering based on particular call \n",
    "There is sometimes a requirement of further analysis and investigation on a call, for e.g. if there is a customer escalation. Call transcripts can be very long and hard to read in raw format. \n",
    "\n",
    "To help us with QnA on a call, we can use LLMs to analyse and answer questions on a call transcript as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "The customer problem was that they were unable to log in to their online account and were also unable to use their debit card. They had tried resetting their password, but the issue persisted.\n"
     ]
    }
   ],
   "source": [
    "insights_prompt_template = \"\"\"\n",
    "Answer the following question: $question\n",
    "\"\"\"\n",
    "\n",
    "question = \"what was the customer problem?\"\n",
    "\n",
    "insights_prompt = Template(insights_prompt_template).safe_substitute(question=question)\n",
    "\n",
    "output_lines = ask_llm(system_prompt, insights_prompt)\n",
    "for line in output_lines:\n",
    "    print(line['text'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
