{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG analysis using Agents\n",
    "\n",
    "In this example, we demonstrate how a generative AI agent can be used to perform ESG analysis. We will use two tools:\n",
    "1. A relational database, which contains the ESG metric data for companies. Here we are using SQLite database (in-memory) with SQLAlchemy as the ORM to interact with the database.\n",
    "2. A vector store which contains annual ESG report data for companies. Here we are using Pinecone serverless.\n",
    "\n",
    "\n",
    "For this example, we are using sample ESG data for fictional companies with the name Company1, Company2 and Company3. ESG reports can be found under ESG-reports folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the requirements\n",
    "\n",
    "The first steps is to install the required libraries in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n",
    "\n",
    "Next, we add the neccessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import insert\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.agents import Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the LLM and embedding model\n",
    "\n",
    "Now we set up the LLM and embedding model.\n",
    "\n",
    "For LLM, we are using Anthropic's Claude 3.5 Sonnet. Our initial tests showed that Llama3 was not producing satisfactory results with ReAct prompting whereas Claude3.5 Sonnet performed the task really well.\n",
    "\n",
    "For embedding model, we are using the open source Hugging Face Sentence-Transformer embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL_ID = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=DEFAULT_MODEL_ID,\n",
    "    temperature=0,\n",
    "    max_tokens=None\n",
    ")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up in memory RDBMS\n",
    "\n",
    "Now we set up the first tool which is RDBMS. We first create the in-memory SQL engine based on SQLite and then create a table ESG_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up in memory RDBMS with Sqqlite and SqlAlchemy as an ORM wrapper\n",
    "#testing only the DB interaction in isolation for now...\n",
    "sql_engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata = MetaData()\n",
    "\n",
    "esg_data = Table(\n",
    "    \"esg_data\",\n",
    "    metadata,\n",
    "    Column(\"instance_id\", Integer, primary_key=True),\n",
    "    Column(\"company_name\", String(4), nullable=False),\n",
    "    Column(\"esg_score\", Float, nullable=False),\n",
    "    Column(\"total_emmisions\", Float, nullable=True),\n",
    "    Column(\"net_zero_target\", Date, nullable=True),\n",
    "    Column(\"renewable_energy_pct\", Float, nullable=True),\n",
    "    Column(\"board_diversity_pct\", Float, nullable=True),\n",
    ")\n",
    "\n",
    "metadata.create_all(sql_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data in ESG_DATA table\n",
    "\n",
    "Next we insert some sample data in ESG table using SQLAlchemy ORM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    [1, 'Company1', 500, 600, datetime(2030,2,1), 40, 50],\n",
    "    [2, 'Company2', 900, 700, datetime(2040,2,15), 30, 60],\n",
    "    [3, 'Company3', 300, 300, datetime(2030, 3,15), 60, 40]\n",
    "]\n",
    "\n",
    "def insert_records(records):\n",
    "    stmt = insert(esg_data).values(\n",
    "    instance_id=records[0],\n",
    "    company_name=records[1],\n",
    "    esg_score=records[2],\n",
    "    total_emmisions=records[3],\n",
    "    net_zero_target=records[4],\n",
    "    renewable_energy_pct=records[5],\n",
    "    board_diversity_pct=records[6]\n",
    "    )\n",
    "\n",
    "    with sql_engine.begin() as conn:\n",
    "        conn.execute(stmt)\n",
    "\n",
    "\n",
    "for record in records:\n",
    "    insert_records(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if records are successfully persisted\n",
    "\n",
    "As a test, we check if the table is correctly populated with the data or not. This is an optional step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = esg_data.select()\n",
    "conn = sql_engine.connect()\n",
    "result = conn.execute(s)\n",
    "\n",
    "for row in result:\n",
    "   print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the database tools, to be used by Agent later\n",
    "\n",
    "Now we prepare the database tool. Note that Langchain provides Toolkit for interacting with RDBMS which contains several tools for fetching data, checking schema etc. We will fetch the pre-built tools from the Toolkit ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase(sql_engine)\n",
    "\n",
    "sql_db_toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "db_tools = sql_db_toolkit.get_tools()\n",
    "\n",
    "db_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Vector store\n",
    "\n",
    "Now, we set up the vector store. Here we are using Pinecone serverless. To use this, you need to register on [pinecone website](app.pinecone.io) and get a free API key. This key can be used to create up to 5 indexes with enough storage to run this example. Replace the placeholder to put your pinecone key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_key = \"<<enter pinecone API key>>\"\n",
    "\n",
    "# configure client\n",
    "pinecone_client = Pinecone(api_key=pinecone_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Delete any pre-existing indexes \n",
    "\n",
    "If you are running this notebook multiple times, run the cell below to delete any pre-existing indexes as Pinecone free serverless edition has a limit of 5 indexes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = pinecone_client.list_indexes()\n",
    "\n",
    "vv = [pinecone_client.delete_index(i.name) for i in indexes] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vector store index\n",
    "\n",
    "Now, we create the vector store index. Note that the dimension of the vector store has to match the dimension of the embedding model which is 768 in this case. We are using cosine similarity as the vector search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the empty index\n",
    "index_name = \"esg-001\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pinecone_client.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pinecone_client.create_index(\n",
    "        index_name,\n",
    "        dimension=768,  # dimensionality of embedding model used\n",
    "        metric='cosine', # or can also try dotproduct\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    # wait for index to be created\n",
    "    while not pinecone_client.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect\n",
    "vector_index = pinecone_client.Index(index_name)\n",
    "time.sleep(3)\n",
    "\n",
    "vector_store = PineconeVectorStore(index=vector_index, embedding=embedding_model)\n",
    "\n",
    "# check index stats -- should be empty for now\n",
    "vector_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents in the vector store\n",
    "\n",
    "Now, we load the documents in the vector store. The ESG reports are present under ESG-reports folder and we load each report one-by-one, chunk it and store the embedding in vector store.\n",
    "\n",
    "We have kept a chunk size of 100 tokens with chunk overlap of 10%. This worked well for these reports. For a different document type, chunking size might need to be changed.\n",
    "\n",
    "similarly, we are using RecursiveCharacterTextSplitter to chunk the document as it worked best for our reports. Based on the document type, you may want to try other splitter classes from Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESG documents\n",
    "\n",
    "def add_document_to_vector_store(document_path):\n",
    "    with open(document_path) as f:\n",
    "        esg_report = f.read()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=10,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    texts = text_splitter.split_documents([Document(esg_report)])\n",
    "\n",
    "    uuids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "    vector_store.add_documents(documents=texts, ids=uuids)\n",
    "\n",
    "\n",
    "add_document_to_vector_store(\"ESG-reports/Company1-ESG-report.txt\")\n",
    "add_document_to_vector_store(\"ESG-reports/Company2-ESG-report.txt\")\n",
    "add_document_to_vector_store(\"ESG-reports/Company3-ESG-report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the documents are indexed\n",
    "\n",
    "We just describe the index to verify if its filled with new vectors or not. The vector count should come around 337."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Vector store tool\n",
    "\n",
    "Final step in vector store set up is to create the tool representation of the vector store so it can be supplied to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# retrieval qa chain\n",
    "vector_store_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add tools to the agent\n",
    "\n",
    "In this step, we merge both the database and vector store tools in a single tool list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_tools = [\n",
    "    Tool(\n",
    "        name = 'VectorStore',\n",
    "        func=vector_store_retriever.run,\n",
    "        description=\"Useful for searching information from the knowledge base. This should be given the first priority while searching for information. It has information about Company1, Company2 and Company3's ESG reports in detail. If the information is not found, then the database tools must be used to find the answer\"\n",
    "    )\n",
    "]\n",
    "\n",
    "tools = rag_tools + db_tools\n",
    "\n",
    "#only needed for Anthropic model\n",
    "llm_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and invoke agent\n",
    " Now we can set up the agent and invoke it with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langsmith import Client\n",
    "\n",
    "# use a prompt template, change it as per use-case.\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "\n",
    "# Construct the Tool Calling Agent\n",
    "agent = create_structured_chat_agent(llm_with_tools, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# provide additional instruction to the agent to leverage all the tools for finding the right answer, before giving up.\n",
    "additional_instructions_for_agent = \"Use all the tools at your disposal to find the right answer for the query below: \\n\\n\"\n",
    "\n",
    "# Query\n",
    "query = additional_instructions_for_agent + \"What is the favorability rating on work-life balance and flexibility for Company1 in the year 2022??\"\n",
    "\n",
    "#invoke the agent\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This concludes the agentic implementation of ESG analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
