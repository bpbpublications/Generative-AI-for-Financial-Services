{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning and domain adaptation for Financial Services \n",
    "\n",
    "In this example we will show how to adapt a popular open source model (Llama 3) towards financial services domain using a method called Domain Adaptaptation or Continued Pre-training. The idea is to take a pre-trained model and further refine it using a domain specific textual dataset. This is an example of unsupervised learning as it does not use examples of inputs and outputs, but rather trains the model to learn the language patterns in a specialised domain such as Financial Services. The expectation is that after fine-tuning our text generation model on the financial documents, the model is able to generate insightful financial related textual output, and therefore can be used to solve multiple domain-specific NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning dataset\n",
    "\n",
    "SEC filings are critical for regulation and disclosure in finance. Filings notify the investor community about companies’ business conditions and the future outlook of the companies. The text in SEC filings covers the entire gamut of a company’s operations and business conditions. For more information on [10-K filings](https://www.investopedia.com/terms/1/10-k.asp) see [How to Read a 10-K](https://www.investor.gov/introduction-investing/general-resources/news-alerts/alerts-bulletins/investor-bulletins/how-read).\n",
    "\n",
    "We will use a subset of SEC filings data of three companies (Amazon, Apple and Meta) for year 2022-2023 in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data are shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this demo notebook, we demonstrate how to fine-tune the Llama-3-8B model using QLoRA, Hugging Face PEFT, and bitsandbytes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Development environment\n",
    "\n",
    "\n",
    "This notebook is using the Hugging Face container for the `us-east-1` region. Make sure you are using the right image for your AWS region, otherwise edit [config.yaml](./config.yaml). Container Images are available [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libriaries, including the Hugging Face libraries, and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n",
    "%pip install -q -U datasets==2.18.0\n",
    "%pip install -q -U langchain==0.1.5\n",
    "%pip install -q -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Amazon SageMaker which allows us to fine-tune the Llama 3 model using HuggingFace libraries. The code itself used HuggingFace libraries which can be used to train locally as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and upload the dataset\n",
    "\n",
    "Read train dataset in a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader, default_header_template\n",
    "\n",
    "annual_reports = [\n",
    "    \"https://www.sec.gov/Archives/edgar/data/1018724/000101872424000008/amzn-20231231.htm\", # Amazon 2023\n",
    "    \"https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20230930.htm\", # Apple 2023\n",
    "    \"https://www.sec.gov/Archives/edgar/data/1326801/000132680124000012/meta-20231231.htm\", # Meta 2023\n",
    "]\n",
    "\n",
    "# SEC website requires specific User Agent to be set with Company Name and Email Address\n",
    "# https://www.sec.gov/os/webmaster-faq#code-support \n",
    "sec_header_template = default_header_template.copy()\n",
    "sec_header_template[\"User-Agent\"] = \"Sample Company Name AdminContact@<sample company domain>.com\"\n",
    "\n",
    "loader = WebBaseLoader(annual_reports, header_template=sec_header_template)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def strip_spaces(doc):\n",
    "    return {\"text\": doc.page_content.replace(\"  \", \"\")}\n",
    "\n",
    "stripped_data = list(map(strip_spaces, data))\n",
    "\n",
    "train_dataset = Dataset.from_list(stripped_data)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Hugging Face Trainer class to fine-tune the model. Define the hyperparameters we want to use. We also create a DataCollator that will take care of padding our inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"<HF_TOKEN>\" # change to your HuggingFace Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating chunks and tokenizing the inputs for making it usable by the LLM. For additional details, please refer to the blog [Leveraging qLoRA for Fine-Tuning of Task-Fine-Tuned Models Without Catastrophic Forgetting: A Case Study with LLaMA2(-chat)](https://medium.com/towards-data-science/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize and chunk dataset\n",
    "chunk_size = 2048\n",
    "\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=chunk_size),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "print(f\"Total number of train samples: {len(lm_train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save lm_train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/sec_data/train'\n",
    "lm_train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Fine-Tune LLaMA 3 8B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [train.py](./train.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "source_dir = \"/home/ec2-user/SageMaker/book/sagemaker/scripts\"\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 10,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # train script\n",
    "    source_dir           = source_dir,        # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',  # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Fine-Tuned model\n",
    "\n",
    "Note: Run `train_fn` with `merge_weights=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_prefix = f'huggingface-qlora-'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_job_name(job_name_prefix):\n",
    "    import boto3\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    search_response = sagemaker_client.search(\n",
    "        Resource='TrainingJob',\n",
    "        SearchExpression={\n",
    "            'Filters': [\n",
    "                {\n",
    "                    'Name': 'TrainingJobName',\n",
    "                    'Operator': 'Contains',\n",
    "                    'Value': job_name_prefix\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'TrainingJobStatus',\n",
    "                    'Operator': 'Equals',\n",
    "                    'Value': \"Completed\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=1)\n",
    "    \n",
    "    return search_response['Results'][0]['TrainingJob']['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = get_last_job_name(job_prefix)\n",
    "\n",
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"1.4\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    model_data=f\"s3://{bucket_name}/{job_name}/output/model.tar.gz\",\n",
    "    env={\n",
    "        'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "        'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "    },\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a base Llama 3 model for comparison\n",
    "\n",
    "Let's also deploy a base model to compare the responses and see the difference after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        'HF_MODEL_ID': \"meta-llama/Meta-Llama-3-8B-Instruct\", # model id within Hugginface Model Hub\n",
    "        'HUGGING_FACE_HUB_TOKEN': HF_TOKEN,\n",
    "    }, # configuration for loading model from Hub\n",
    "    role=role, # iam role with permissions to create an Endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictor = base_model.deploy(\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) If the model is predeployed, you can connect to it using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "base_predictor = Predictor(\n",
    "    endpoint_name=\"huggingface-pytorch-tgi-inference-2024-06-29-12-26-05-297\", # change to endpoint name\n",
    "    sagemaker_session=sess,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "We can now query the model. We will form some sample queries using the [Llama 3 prompting format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>{{question}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = base_prompt.format(question=\"What drives sales growth at Amazon?\")\n",
    "\n",
    "response = predictor.predict({\n",
    "\t\"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 1000 - len(prompt),\n",
    "        \"temperature\": 0.1,\n",
    "        # \"top_p\": 0.9,\n",
    "    }\n",
    "})\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance with base model\n",
    "\n",
    "Now let's compare the results we get from base model for the same set of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining a utility function for querying our endpoint\n",
    "def query_llama3(question, predictor):\n",
    "    prompt = base_prompt.format(question=question)\n",
    "\n",
    "    response = predictor.predict({\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 2048 - len(prompt),\n",
    "            \"temperature\": 0.01,\n",
    "            \"top_k\": 250,\n",
    "            \"top_p\": 0.8,\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # \"How did the COVID-19 pandemic impact Amazon’s business?\",\n",
    "    \"What is Apple's strategy for growth in Asia\",\n",
    "    \"What are Meta's plans to invest more in AI and Generative AI?\",\n",
    "    # \"What was Amazon's net sales for year 2023?\",\n",
    "    \"What are the key priorities for Amazon in 2023?\",\n",
    "]\n",
    "for q in questions:\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"-\"*40 + \"BASE MODEL\" + \"-\"*40)\n",
    "    print(q)\n",
    "    print(query_llama3(q, base_predictor))\n",
    "    print(\"-\"*40 + \"FINE-TUNED MODEL\" + \"-\"*40)\n",
    "    print(query_llama3(q, predictor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "\n",
    "base_predictor.delete_model()\n",
    "base_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
